{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import AdamW\n",
        "import pandas as pd\n",
        "import logging\n",
        "import os"
      ],
      "metadata": {
        "id": "rVJYV5j-9YcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class COVIDTweetClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=5, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.feature_layer = nn.Sequential(\n",
        "            nn.Linear(768 + 3, 512),  # BERT dim + 3 handcrafted features\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.classifier = nn.Linear(512, num_classes)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, features):\n",
        "        bert_output = self.bert(input_ids, attention_mask).last_hidden_state[:, 0, :]\n",
        "        combined = torch.cat([bert_output, features], dim=1)\n",
        "        return self.classifier(self.feature_layer(combined))\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_features(text):\n",
        "        \"\"\"Handcrafted features matching training feature extraction\"\"\"\n",
        "        return torch.tensor([\n",
        "            len(text.split()) / 100,        # Normalized word count\n",
        "            text.count('!') + text.count('?'),  # Punctuation intensity\n",
        "            1 if 'http' in text else 0     # URL presence\n",
        "        ], dtype=torch.float32)\n",
        "\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, texts, features, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            self.texts[idx],\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'features': self.features[idx],\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2Qyh76Nptd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_path):\n",
        "    df = pd.read_csv(data_path, encoding='latin1')\n",
        "    df['text'] = df['OriginalTweet'].str.strip()\n",
        "    df['label'] = df['Sentiment'].map({\n",
        "        'Extremely Negative': 0, 'Negative': 1,\n",
        "        'Neutral': 2, 'Positive': 3, 'Extremely Positive': 4\n",
        "    })\n",
        "    features = [COVIDTweetClassifier.extract_features(text).numpy() for text in df['text']]\n",
        "    return df['text'].tolist(), torch.tensor(features), df['label'].tolist()\n"
      ],
      "metadata": {
        "id": "VJTAuNZVpw8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train():\n",
        "    # Load data\n",
        "    train_texts, train_features, train_labels = load_data('Corona_NLP_train.csv')\n",
        "    test_texts, test_features, test_labels = load_data('Corona_NLP_test.csv')\n",
        "\n",
        "    # Prepare datasets\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    train_dataset = TweetDataset(train_texts, train_features, train_labels, tokenizer)\n",
        "    test_dataset = TweetDataset(test_texts, test_features, test_labels, tokenizer)\n",
        "\n",
        "    # Training setup\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = COVIDTweetClassifier().to(device)\n",
        "    model = nn.DataParallel(model)  # Enable multi-GPU support\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "    # Early stopping parameters\n",
        "    best_f1 = 0\n",
        "    patience = 2\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(3):  # Reduced to 3 epochs for runtime optimization\n",
        "        model.train()\n",
        "        logging.info(f'Starting epoch {epoch+1}')\n",
        "        for batch in train_loader:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**inputs)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "                outputs = model(**inputs)\n",
        "                all_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "                all_labels.extend(batch['labels'].numpy())\n",
        "\n",
        "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "        logging.info(f\"Epoch {epoch+1} | Test F1: {f1:.3f}\")\n",
        "\n",
        "        # Check for early stopping\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), 'covid_model.pth')\n",
        "            logging.info('Model improved and saved.')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            logging.info('No improvement.')\n",
        "            if patience_counter >= patience:\n",
        "                logging.info('Early stopping triggered.')\n",
        "                break\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzxHDViScC2K",
        "outputId": "5c5047c4-6239-4fca-fa0d-42d7cd1f4904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-459a6e78689b>:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return df['text'].tolist(), torch.tensor(features), df['label'].tolist()\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "class TweetPredictor:\n",
        "    def __init__(self, model_path='covid_model.pth'):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = COVIDTweetClassifier().to(self.device)\n",
        "\n",
        "        # Fix 1: Handle DataParallel weights and security warning\n",
        "        state_dict = torch.load(model_path, map_location=self.device, weights_only=True)\n",
        "\n",
        "        # Fix 2: Remove 'module.' prefix from DataParallel-trained weights\n",
        "        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
        "\n",
        "        self.model.load_state_dict(state_dict)\n",
        "        self.model.eval()\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.class_map = {\n",
        "            0: 'Extremely Negative', 1: 'Negative',\n",
        "            2: 'Neutral', 3: 'Positive', 4: 'Extremely Positive'\n",
        "        }\n",
        "\n",
        "    def predict(self, text):\n",
        "        features = COVIDTweetClassifier.extract_features(text).to(self.device)\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            max_length=128,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.model(encoding['input_ids'], encoding['attention_mask'], features.unsqueeze(0))\n",
        "\n",
        "        return self.class_map[output.argmax().item()]\n",
        "\n",
        "#Example\n",
        "if __name__ == '__main__':\n",
        "    predictor = TweetPredictor()\n",
        "    print(predictor.predict(\"Vaccine distribution is going great!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUnUb3A_dmyR",
        "outputId": "d6dcb7c1-7cd6-4ab1-e2c2-c2c96810e7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extremely Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install onnxruntime\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "            outputs = model(**inputs)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    cls_report = classification_report(all_labels, all_preds,\n",
        "                                         target_names=['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive'])\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    return acc, precision, recall, f1, cls_report, conf_matrix\n",
        "\n",
        "def main_evaluation():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # Load your test data (ensure you have test_texts, test_features, test_labels)\n",
        "    test_texts, test_features, test_labels = load_data('Corona_NLP_test.csv')\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    test_dataset = TweetDataset(test_texts, test_features, test_labels, tokenizer)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "    # Load the trained model\n",
        "    model = COVIDTweetClassifier().to(device)\n",
        "    state_dict = torch.load('covid_model.pth', map_location=device)\n",
        "    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    acc, precision, recall, f1, cls_report, conf_matrix = evaluate_model(model, test_loader, device)\n",
        "\n",
        "    print(f\"Test Accuracy: {acc:.3f}\")\n",
        "    print(f\"Test Precision: {precision:.3f}\")\n",
        "    print(f\"Test Recall: {recall:.3f}\")\n",
        "    print(f\"Test F1 Score: {f1:.3f}\")\n",
        "    print(\"\\nClassification Report:\\n\", cls_report)\n",
        "    print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main_evaluation()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__AEiKCvJmyR",
        "outputId": "08197636-58c9-4766-833f-af638990d9ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-aad2927780f6>:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('covid_model.pth', map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.852\n",
            "Test Precision: 0.855\n",
            "Test Recall: 0.852\n",
            "Test F1 Score: 0.853\n",
            "\n",
            "Classification Report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "Extremely Negative       0.90      0.84      0.87       592\n",
            "          Negative       0.82      0.86      0.84      1041\n",
            "           Neutral       0.94      0.85      0.89       619\n",
            "          Positive       0.80      0.84      0.82       947\n",
            "Extremely Positive       0.86      0.88      0.87       599\n",
            "\n",
            "          accuracy                           0.85      3798\n",
            "         macro avg       0.87      0.85      0.86      3798\n",
            "      weighted avg       0.85      0.85      0.85      3798\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[499  89   1   3   0]\n",
            " [ 52 893  22  72   2]\n",
            " [  1  45 524  49   0]\n",
            " [  2  58  11 794  82]\n",
            " [  0   2   0  71 526]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeERY96qMftF",
        "outputId": "ba590029-4c9b-4011-fa42-eddefa6ffb8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.25.5)\n",
            "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def export_to_onnx(model, dummy_input, onnx_file_path='covid_model.onnx'):\n",
        "    model.eval()\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        dummy_input,\n",
        "        onnx_file_path,\n",
        "        input_names=['input_ids', 'attention_mask', 'features'],\n",
        "        output_names=['output'],\n",
        "        dynamic_axes={\n",
        "            'input_ids': {0: 'batch_size'},\n",
        "            'attention_mask': {0: 'batch_size'},\n",
        "            'features': {0: 'batch_size'},\n",
        "            'output': {0: 'batch_size'}\n",
        "        },\n",
        "        opset_version=14\n",
        "    )\n",
        "    print(f\"Model exported to {onnx_file_path}\")\n",
        "\n",
        "# Example usage\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = COVIDTweetClassifier().to(device)\n",
        "state_dict = torch.load('covid_model.pth', map_location=device)\n",
        "state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "dummy_input_ids = torch.zeros(1, 128, dtype=torch.int64).to(device)\n",
        "dummy_attention_mask = torch.zeros(1, 128, dtype=torch.int64).to(device)\n",
        "dummy_features = torch.zeros(1, 3, dtype=torch.float32).to(device)\n",
        "\n",
        "export_to_onnx(model, (dummy_input_ids, dummy_attention_mask, dummy_features))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mYs_p5rMD7D",
        "outputId": "7b3bcae8-633a-450c-c236-80c5886e15f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-b2611fe06ccd>:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('covid_model.pth', map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model exported to covid_model.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "class ONNXTweetPredictor:\n",
        "    def __init__(self, onnx_model_path='covid_model.onnx'):\n",
        "        self.session = ort.InferenceSession(onnx_model_path)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.class_map = {\n",
        "            0: 'Extremely Negative', 1: 'Negative',\n",
        "            2: 'Neutral', 3: 'Positive', 4: 'Extremely Positive'\n",
        "        }\n",
        "\n",
        "    def predict(self, text):\n",
        "        features = COVIDTweetClassifier.extract_features(text).numpy()\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            max_length=128,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='np'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].astype(np.int64)\n",
        "        attention_mask = encoding['attention_mask'].astype(np.int64)\n",
        "        features = features.astype(np.float32).reshape(1, -1)\n",
        "\n",
        "        outputs = self.session.run(\n",
        "            None,\n",
        "            {\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': attention_mask,\n",
        "                'features': features\n",
        "            }\n",
        "        )\n",
        "\n",
        "        return self.class_map[np.argmax(outputs[0])]\n",
        "\n",
        "# Example usage\n",
        "onnx_predictor = ONNXTweetPredictor()\n",
        "print(onnx_predictor.predict(\"Vaccine distribution is going great!\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH-BWKNwMLAr",
        "outputId": "389a8e2e-554f-4c06-d927-4316286d2a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extremely Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "W8hagWp7PBEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some literature have published that random forest could have a good results. So, I again used BERT to tokenize our text and then use random forest to train the model. Unfortunately, we can see that the results are not as good as the neural network. However, I included my code and thought process below.\n",
        "\n",
        "一些文献表明，随机森林可能会取得不错的结果。因此，我再次使用 BERT 对文本进行分词，然后使用随机森林来训练模型。不幸的是，我们可以看到结果不如神经网络。然而，我在下面包含了我的代码和思路。"
      ],
      "metadata": {
        "id": "VT9fHEXHmD-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np  # Ensure numpy is imported\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (accuracy_score, f1_score, classification_report,\n",
        "                             confusion_matrix, precision_score, recall_score)\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "uzJ4JwWkpghq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class COVIDTweetClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=5, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.feature_layer = nn.Sequential(\n",
        "            nn.Linear(768 + 3, 512),  # BERT dim + 3 handcrafted features\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.classifier = nn.Linear(512, num_classes)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, features):\n",
        "        bert_output = self.bert(input_ids, attention_mask).last_hidden_state[:, 0, :]\n",
        "        combined = torch.cat([bert_output, features], dim=1)\n",
        "        return self.classifier(self.feature_layer(combined))\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_features(text):\n",
        "        \"\"\"Handcrafted features matching training feature extraction\"\"\"\n",
        "        return torch.tensor([\n",
        "            len(text.split()) / 100,         # Normalized word count\n",
        "            text.count('!') + text.count('?'), # Punctuation intensity\n",
        "            1 if 'http' in text else 0         # URL presence\n",
        "        ], dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "6JeQRcaFpnTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, texts, features, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            self.texts[idx],\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'features': self.features[idx],\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n"
      ],
      "metadata": {
        "id": "GXnMrm8XppdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_path):\n",
        "    df = pd.read_csv(data_path, encoding='latin1')\n",
        "    df['text'] = df['OriginalTweet'].str.strip()\n",
        "    df['label'] = df['Sentiment'].map({\n",
        "        'Extremely Negative': 0, 'Negative': 1,\n",
        "        'Neutral': 2, 'Positive': 3, 'Extremely Positive': 4\n",
        "    })\n",
        "    features = [COVIDTweetClassifier.extract_features(text).numpy() for text in df['text']]\n",
        "    return df['text'].tolist(), torch.tensor(features), df['label'].tolist()\n",
        "\n",
        "def train_random_forest():\n",
        "    texts, features, labels = load_data('Corona_NLP_train.csv')\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    dataset = TweetDataset(texts, features, labels, tokenizer)\n",
        "    data_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = COVIDTweetClassifier().to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Extract BERT features\n",
        "    all_features = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            handcrafted_features = batch['features'].to(device)\n",
        "            bert_output = model.bert(input_ids, attention_mask).last_hidden_state[:, 0, :]\n",
        "            combined_features = torch.cat([bert_output, handcrafted_features], dim=1)\n",
        "            all_features.append(combined_features.cpu().numpy())\n",
        "\n",
        "    all_features = np.vstack(all_features)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Split data for training and validation\n",
        "    X_train, X_val, y_train, y_val = train_test_split(all_features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train Random Forest\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate Random Forest\n",
        "    y_pred = rf_classifier.predict(X_val)\n",
        "    acc = accuracy_score(y_val, y_pred)\n",
        "    precision = precision_score(y_val, y_pred, average='weighted')\n",
        "    recall = recall_score(y_val, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
        "    cls_report = classification_report(\n",
        "        y_val, y_pred,\n",
        "        target_names=['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']\n",
        "    )\n",
        "    conf_matrix = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "    # Print out the metrics\n",
        "    print(f\"Random Forest Accuracy: {acc:.3f}\")\n",
        "    print(f\"Random Forest Precision: {precision:.3f}\")\n",
        "    print(f\"Random Forest Recall: {recall:.3f}\")\n",
        "    print(f\"Random Forest F1 Score: {f1:.3f}\")\n",
        "    print(\"\\nClassification Report:\\n\" + cls_report)\n",
        "    print(\"\\nConfusion Matrix:\\n\" + str(conf_matrix))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_random_forest()\n"
      ],
      "metadata": {
        "id": "1OAutOWoeKuF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}