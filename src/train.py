# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tw0a9mYOYYYsL40Zzg3GDg0hzitI6DOB
"""

### train.py
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import BertTokenizer, AdamW
import logging
from dataset import TweetDataset, load_data
from model import COVIDTweetClassifier

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def train():
    train_texts, train_features, train_labels = load_data('Corona_NLP_train.csv')
    test_texts, test_features, test_labels = load_data('Corona_NLP_test.csv')

    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    train_dataset = TweetDataset(train_texts, train_features, train_labels, tokenizer)
    test_dataset = TweetDataset(test_texts, test_features, test_labels, tokenizer)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = COVIDTweetClassifier().to(device)
    model = nn.DataParallel(model)
    optimizer = AdamW(model.parameters(), lr=2e-5)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64)

    for epoch in range(3):
        model.train()
        for batch in train_loader:
            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
            labels = batch['labels'].to(device)
            optimizer.zero_grad()
            outputs = model(**inputs)
            loss = nn.CrossEntropyLoss()(outputs, labels)
            loss.backward()
            optimizer.step()

if __name__ == '__main__':
    train()