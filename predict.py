# -*- coding: utf-8 -*-
"""predict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XNcRok3jL9Jwl0VUuhQ6oMAKE2VTOWbr
"""

import torch
from transformers import BertTokenizer
from model import COVIDTweetClassifier

class TweetPredictor:
    def __init__(self, model_path='covid_model.pth'):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = COVIDTweetClassifier().to(self.device)

        state_dict = torch.load(model_path, map_location=self.device)
        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}

        self.model.load_state_dict(state_dict)
        self.model.eval()

        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.class_map = {
            0: 'Extremely Negative', 1: 'Negative',
            2: 'Neutral', 3: 'Positive', 4: 'Extremely Positive'
        }

    def predict(self, text):
        features = COVIDTweetClassifier.extract_features(text).to(self.device)
        encoding = self.tokenizer.encode_plus(
            text,
            max_length=128,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        ).to(self.device)

        with torch.no_grad():
            output = self.model(encoding['input_ids'], encoding['attention_mask'], features.unsqueeze(0))

        return self.class_map[output.argmax().item()]

if __name__ == '__main__':
    predictor = TweetPredictor()
    #print(predictor.predict("Vaccine distribution is going great!"))